\documentclass{report}

\usepackage[headings]{fullpage}
%\usepackage{graphicx}
\usepackage{scribe}

% Packages
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{epsfig}
\usepackage{color}
\usepackage{array}
\usepackage{pstricks}
\usepackage{pst-plot}
%\usepackage{pstricks-add}
\usepackage{multirow}

\theoremstyle{plain}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{prop}[lemma]{Property}
\newtheorem{fact}[lemma]{Fact}

\theoremstyle{definition}
\newtheorem{define}[lemma]{Definition}
\newtheorem{example}[lemma]{Example}

\newtheorem{problem}{Problem}

\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\var}{\mbox{\rm var}}
\newcommand{\pr}{\mbox{\rm Pr}}

\def\X{{\cal X}}
\def\cost{{\mbox{\rm cost}}}
\def\U{{\cal U}}

\begin{document}

\course{CSE 103}
\coursetitle{Probability and statistics}
\semester{Fall 2013}
\lecturer{Yoav Freund}
\scribe{}
\lecturenumber{6}
\lecturetopic{CDFs and  Sorting}

\maketitle

\section{Cumulative Distribution Functions}

Our discussion so far focused on finite event spaces or event spaces
that correspond to the integers $0,1,2,\ldots$ (so-called countable
infinite sets). How do we define distributions over the real numbers?
That is an {\em uncountably} infinite set.\footnote{A set $A$ is
  uncountable if there is no one-to-one mapping from $A$ to the
  positive integers $1,2,3,4,...$. In other words, a set is
  uncountable if you cannot create a list which includes all of the
  elements in the set.}

When defining a distribution over the real it is not enough to assign
probabilities to individual points on the real line. Consider the {\em
  uniform distribution on the line segment $[0,1]$}, by which
we mean the set $\{x | 0 \leq x \leq 1\}$. We cannot assign each
point a probability larger than zero. because that would result in
$[0,1]$ an infinite probability. What we do instead is assign
probability to the line segment $0 \leq a < b \leq 1$ the probability
$b-a$. Thus for example $P([1/4,1/3])=1/3-1/4 = 1/12$.

More generally, we can define any distribution over the real line
using the {\em Cumulative Distribution Function}:
\newcommand{\CDF}{\mbox{CDF}}
\[
\CDF(a) \doteq P(x \leq a)
\]

\begin{figure}[th]
\begin{center}
\includegraphics[width=5in]{figs/CDFmapping.png}
\end{center}
\caption{This figure depicts an example of mapping a random variable
  $X$ whose distribution is defined by a CDF (thick black curve) to
  another random variable $Y$, whose distribution is uniform in the
  interval $[0,1]$. The mapping uses the CDF as a function so that
  $Y=\CDF(X)$. \label{fig:CDFmap}}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=5in]{figs/CDFs.png}
\end{center}
\caption{The CDFs for two distributions: (1) A point-mass distribution
  is a distribution that assignes non zero probabilities to two real
  values (marked by red circles) all sets that do not include at least
  one of these points have probability zero. The CDF of a point mass
  distribution is constant every where but at the point masses where
  it is discontinuous, the size of the discontinuity is the
  probability of the corresponding point. (2) A density distribution
  assigns probability zero to any single point. The CDF for such a
  distribution is a continuous increasing function that has a
  derivative. This derivative, $p(a)$ is called the {\em density function} of
  the distribution. For density distributions the CDF and the density
  function contain the same information.\label{fig:CDF}}
\end{figure}

As we see in Figure~\ref{fig:CDF} the CDF is an increasing function that
increases from zero at $-\infty$ to one at $+\infty$.

It is easy to see that $P(a<x\leq b)=\CDF(b)-\CDF(a)$.


\section{Examples of distributions on the real line}
If the distribution assigns a non-zero probability to some $x=a$ we
say that the distribution has a {\em point mass} at $a$. In that case
the $\CDF$ has a jump at $a$. We denote a point mass distribution
concentrated at the point $a$ by $PM(a)$. The distribution $PM(a)$
corresponds to a random variable such that $P(X=a)=1$.

\begin{figure}[t]
\begin{center}
\includegraphics[width=3in]{figs/Discrete1.jpg}
\includegraphics[width=3in]{figs/4PointMass.jpg}
\end{center}
\caption{{\bf Left:} A discrete distribution concentrated on a single
  point $P(X=2)=1$. We denote this distribution by $PM(2)$.  {\bf
    Right:} A discrete distribution distributed evenly over the four
  points $-1,0,1,2$. This distribution can be expressed as $(PM(-1)+PM(0)+PM(1)+PM(2))/4$.}
\end{figure}

\begin{figure}[b]
\begin{center}
\includegraphics[width=3in]{figs/Discrete2.jpg}
\includegraphics[width=3in]{figs/Normal.jpg}
\end{center}
\caption{{\bf Left:} A non-uniform discrete distribution. This
  distribution can be expressed as
  $(1/4)PM(-1)+(1/8)PM(0)+(5/8)PM(1)+(1/4)PM(2)$. {\bf Right:} The
  normal distribution with mean $0$ and varriance 1, denoted ${\cal
  N}(0,1)$. This is a density distribution and it's density function
is $f(x) = \frac{1}{\sqrt{2\pi}} \exp(-x^2/2)$.}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=3in]{figs/Uniform.jpg}
\includegraphics[width=3in]{figs/unifMixture2CDF.jpg}
\end{center}
\caption{{\bf Left:} A uniform distribution between $-3$ and $1$. We
  denote this distribution by $U(-3,1)$. {\bf Right:} A mixture of two
  uniform distributions: $(U(-3,1)+U(2,3))/2$.}
\end{figure}

\[
f(x) = \begin{cases}
0.25 & \mbox{if $ -3 \leq x \leq 1$} \\
0 & \mbox{otherwise}
\end{cases}
\]

Another important case is when the deriveative of the CDF is deifined
$p(a) = \frac{d}{dx}\left|_{x=a} \CDF(x)\right.$. The function $p(a)$
is called the {\em probability density}. Note that if $p(a)$ is
defined then, regarless of how large $p(a)$ is, $P(x=a)=0$.

When a distribution over the reals is a density distribution we can
calculate the probability of the segment $[a,b]$ using the integral:
\[
P(a < x \leq b)=\CDF(b)-\CDF(a)=\int_a^b p(x) dx
\]

\section{Worst-case lower bound on sorting}
You probably know of some algorithms for sorting $n$ numbers in
$O(n\log n)$ time. QuickSort and MergeSort are two such algorithms.

However, did you know that $n \log n$ is a lower bound on {\em any} sorting
algorithm? There is no sorting algorithm that can sort {\em any}
sequence of $n$ elements in time $o(n\log n)$. 

The argument is pretty simple. Let us restrict our attention to a
sequence of length $n$ that consists of the numbers $1,\ldots,n$
appearing in some order,  in other words, some permutation of the
numbers $1,\ldots,n$. As we have shown earlier in the class, there are
$n!$ such permutations.

An algorithm which sorts each of these sequences must be able to
distinguish each one of the $n!$ permutations. Suppose that the
algorithm proceeds by comparing pairs of elements in the
sequence. Each comparison has two possible outcomes. The result is a
binary tree where internal nodes correspond to comparisons and the
leaves correspond to permutations.

\begin{center}
\includegraphics[width=4in]{figs/SortingLowerBound.png}
\end{center}

The depth $d$ of the tree is the worst case number of comparisons done by
the algorithm and therefor a lower bound on the worst case running
time. A basic fact about binary trees is that the number of leaves is
at most $2^d$. We there for have that $n! \leq 2^d$. Taking the
natural log of both sides and using the Strling approximation of 
$n!$ (google it!) we
find that $d \ln 2 \geq \ln n! \geq (n-1) \ln n$. We have thus found that the
worst case running time of any sorting algorithm is $\Omega(n \log n)$.

This shows that the worst case running time is $\Omega(n \log
n)$. However, as we shall see, there are sorting algorithms that,
under some statistical assumptions, take time O(n) in expectation.

\section{Sorting in expected linear time}

Suppose we want to sort an array of numbers $S[1\cdots n]$ that we expect to be
distributed uniformly in some range $[\min,\max]$. Here's a {\it bucket sort} approach:
\begin{itemize}
\item Divide $[\min,\max]$ into $n$ equal-sized intervals. These are the {\it buckets} 
$B_1, B_2, \ldots, B_n$.
\item Now scan array $S$ from left to right, putting each element $S[i]$ in its
appropriate bucket.
\item Return $\mbox{sort}(B_1) \circ \mbox{sort}(B_2) \circ \cdots \circ \mbox{sort}(B_n)$,
where ``sort'' is a standard sorting algorithm (say mergesort).
\end{itemize}

Notice that there is no randomization in the algorithm. However, we can talk about
the expected running time if the elements of $S$ are generated from a uniform 
distribution over $[\min,\max]$. In that case, each element is equally likely to 
fall into any of the buckets $B_i$.

Let $N_i$ be the number of array elements that fall into $B_i$. Assuming
we use a standard sorting procedure for each bucket, we get a total running time of
$$ T = N_1 \log N_1 + N_2 \log N_2 + \cdots + N_n \log N_n \ \leq \ 
N_1^2 + N_2^2 + \cdots + N_n^2 .$$

What is $\E(N_i^2)$? The easiest way to compute this is to write $N_i$ as a sum:
$$ N_i = X_1 + X_2 + \cdots + X_n$$
where $X_j$ is 1 if the array element $S[j]$ falls into bin $i$, and 0 otherwise.
Notice that $X_j^2 = X_j$, and that $X_j$ is independent of $X_{j'}$ whenever $j \neq j'$.
Therefore,
\begin{eqnarray*}
\E(X_j)  &  = & \frac{1}{n} \\
\E(X_j^2) & = & \frac{1}{n} \\
\E(X_jX_j') & = & \E(X_j) \E(X_{j'}) \ \ = \ \ \frac{1}{n^2} \mbox{\ \ \ \ if $j \neq j'$}
\end{eqnarray*}
By linearity of expectation, we then have
\begin{eqnarray*}
\E(N_i^2) 
& = & \E \left( (X_1 + \cdots + X_n)^2 \right) \\
& = & \E \left( \sum_j X_j^2 + \sum_{j \neq j'} X_j X_{j'} \right) \\
& = & \sum_j \E(X_j^2) + \sum_{j \neq j'} \E(X_j X_{j'}) \\
& = & n \cdot \frac{1}{n} + n(n-1) \frac{1}{n^2} \ \ \leq \ \ 2.
\end{eqnarray*}

So the expected running time of the sorting algorithm, once again invoking linearity, is
$$ \E(T) 
\ \leq \ \E(N_1^2) + \E(N_2^2) + \cdots + \E(N_n^2) 
\ \leq \ 2n
.$$
It is linear!

\section{Sorting in linear time when the distribution is known}

In the previous section we described an algorithm that can sort
elements drawn IID from the uniform distribution in expected linear
time. In this section we show how to generalize this to sorting
elements drawn IID from an arbitrary density function over the reals.

Suppose we use the CDF as a transformation, in other words, map each
$X$ to $Y=\CDF(X)$. $Y$ is a new random variable, see
figure~\ref{fig:CDFmap}. What is the distribution of the random
variable $Y$? First, it is clear that $0 \leq Y \leq 1$ because that
is the range of cumulative distribution function. We need to also
assume that the CDF is a {\em reversible} function. I.e. for any real
in the range: $0<c<1$ there exists an inverse $b=\CDF^{-1}(c)$ such
that $\CDF(b)=c$. A sufficient condition for this to hold is that the
distribution over $R$ is defined by a density function.

To understand the distribution of $Y$, let us calculate the
probability that $Y$ is in some range $c<Y<d$, where $0<c<d<1$.
Using the definition of $\CDF^{-1}$ we get
\begin{equation} \label{eqn:CDF}
P(c \leq Y \leq d) = P(\CDF^{-1}(c) \leq X \leq \CDF^{-1}(d)) =
\CDF(\CDF^{-1}(d))-\CDF(\CDF^{-1}(c)) = d-c
\end{equation}
Where the first equality is justified by the definition of
$\CDF^{-1}$, the second by the formula for calculating the probability
of a segment using the CDF, and the fourth by the cancellation :
$\CDF^{-1}(\CDF(X))=X$.  We find that the distribution of $Y$ is
uniform between 0 and 1, under the condition that the CDF is invertible.

It might help to consider a particular CDF as an example. Suppose the
CDF of $X$ is $\CDF(x) = \frac{1}{1+e^{-x}}$, this function is
invertible and it's inverse is
$\CDF^{-1}(y)=\ln\left(\frac{1}{y}-1\right)$. Apply the steps of
Equation~(\ref{eqn:CDF}) to convince yourself the it works.

Recall that we have an efficient algorithm for sorting numbers that
are distributed uniformly in some segment $[\min,\max]$.
If we know the CDF of the distribution that is generating the
numbers we wish to sort, we can map these numbers to the rannge
$[0,1]$ and then use the method suggested in the first section to sort
them in $O(N)$ time.

\end{document}



