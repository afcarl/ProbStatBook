\chapter{Pseudo-randomness}

\section{Randomness and predictability}

Consider a sequence of binary random variables: $X_1,X_2,\ldots,X_n$.
Where each random variable $X_i$ is equal to 1 or to 0 each with
probability $1/2$.

If the random variables are {\em independent} of each other, then the
probability of each of the possible $2^n$ binary sequences is equal to
$2^{-n}$. This is the {\em uniform} distribution over the outcome space.

Suppose that we observe the sequence one bit at a time. Specifically,
suppose we observed $X_1=x_1,X_2=x_2,\ldots,X_i=x_i$ for some $i<n$,
can we predict the value of $X_{i+1}$? The answer is no, because 
\[
P\left(X_{i+1} =x_{i+1} |
  X_1=x_1,X_2=x_2,\ldots,X_i=x_i\right)=\frac{1}{2}
\]
In other words, knowing the values of past bits does not provide
information about future bits.

Thus independence, uniform distribution and unpredictability are all
equivalent. 

We say that a random sequence generator is {\em perfect} if the
for each $i<n$, $X_{i+1}$ is independent from $X_1,\ldots,X_{i}$.
This is equivalent to saying that for any random variable $Y_i$ that is
some function of $X_1,\ldots,X_{i}$, $P(X_{i+1}=1|Y_i=y)=1/2$. The
counter-positive is that if there exists some function of the past bits
for which $P(X_{i+1}|Y_i)\neq 1/2$ then we the sequence is (slightly) 
predictable and therefor the source is not a perfect random bits
generator.

As we will learn in a little while, sequences of random bits are very
useful for computations. Therefor computers need fast random bit
generators. Coin flipping is a possibility, but it is too slow
for computer applications. There are other physical sources of
randomness such as Zener diodes~\footnote{For more information on
  physical random bit generators see
%\verbatim{http://en.wikipedia.org/wiki/Random_number_generation\#Physical_methods}
}
which are much faster than coin flips but suffer from other
limitations.

The approach usually taken to generating random bits in a computer is
to use a {Pseudo-random number generator}.

\section{Pseudo-Randomness}

Pseudo-random number generators are computer programs. As such, their
output is complete predictable from their input. So in what sense is a
pseudo-random number generator random? Lets first define exactly what
we mean by a random number generator.

A pseudo-random number generator can be described as a class withs
three methods: {\tt setState, advance, generate}, and a shared {\tt
  state}. The method {\tt setState} receives as input a {\tt seed} and
sets {\tt state} to {\tt seed}. The internal method {\tt advance}
updates {\tt seed} using some fixed function {\tt F(State) ->
  state}. Finally {\tt generate} calls {\tt advance} and then computes
one bit as the function of the new state and outputs it as the next
random bit.\footnote{In practice, random number generators output
  several bits each time the are called.} The typical use pattern of a
random number generator is to call {\tt setState} once at
initialization and then repeatedly call {\tt generate}.

So, in what sense is a pseudo-random generator actually random?
Clearly, if we know the {\tt seed} then we can recreate the sequence
of generated numbers exactly, so in that sense the sequence is nothing
but random! The key is to argue about a situation in which we don't
know the seed.

\subsection{Predicting a pseudo-random generator}
OK, so suppose that we know the code of the generator but we don't
know the seed with which it was initialized. We call this generator
the ``hidden seed'' generator or $HS$ for short. Can we predict
the next bit in this pseudo-random stream?

As we know the code of the generator we know the number of bits used
in the seed. We call this number $l$ and observe that there are $2^l$
different possible seeds. Suppose we instantiate $2^l$ instances of
the pseudo-random algorithm and initialize each with a different
seed. We denote the instance with seed $i$ $S(i)$. Our prediction
algorithm maintains a set of instances which we call the ``pool''.  We
initialize the pool to contain all $2^l$ instances
$S(1),S(2),\ldots,S(2^l)$.

As $HS$ is also using the same code with some length $l$ seed we know
that there is an instance in the pool which predicts the sequence
perfectly. Had we know which one it is, we could perfectly simulate
$HS$. However, initially we don't have this information. The idea of
the algorithm is that each time we make an incorrect prediction the
size of the pool will decrease by a factor of two the therefor, as we
know that there is at least one element in the pool that never makes a
mistake. The number of mistakes of the prediction algorithm will be at
most $l$.

In some more detail, suppose the bit sequence generated by $HS$ is
$b_1,b_2,b_3,\ldots$. After observing $b_t$ and before observing
$b_{t+1}$ the prediction algorithm does the following:

\begin{enumerate}
\item Remove from the pool all generators that disagree with $b_t$.
\item Predict $b_{t+1}$ according to the majority of the $t+1$'th bit
  generated by the instances in the pool.
\end{enumerate}

The analysis is simple: if the prediction algorithm makes a mistake on
step 2, then at least half of the instances in the pool made a
mistake. These instances will be removed in the following step
1. Therefor the size of the pool is halved each time the prediction
algorithm makes a mistake. As the initial size is $2^l$ and the final
pool cannot be empty (because $HS$ is an element in the pool) the
total number of mistakes is at most $l$. Thus is we generate a
sequence of length ,say, $l^2$ we will easily see that it is not a
truly random sequence.

\subsection{Computationally bounded randomness}
In the previous section we showed that, in some sense,
pseudo-randomness is no randomness at all. Given enough computational
resources you can discriminate between a true random source and a
pseudo random source. However, the key word here is {\em sufficient}
computational resources. Think of actually implementing the algorithm
described in the previous section for an algorithm that uses a seed of
256 bits, which is a reasonable size seed for a pseudo-random number generator.

We need to maintain a pool of $2^{256} > 10^{25}$ elements, that is a
lot of memory space!

So the actual definition of pseudo-random number generator is one such
that for most seed values, the generated sequence cannot be
distinguished from a truly random sequence using space and time that is
polynomial in $l$.

It is worth observing that what we are looking for is a statistical
test in the sense that was described in earlier lessons. Specifically
we have as a null hypothesis that the sequence corresponds to IID RVs
(Identically Distributed and Independent Random Variables) with equal
probabilities for 0 and 1. The goal of the test is to reject the null
hypothesis. Furthermore, as computer scientists, we want the test to
be computationally efficient.

To summarize, pseudo-random number generators generate sequences of
bits that cannot be distinguished from truly random sequences by any
computationally efficient tests.


\subsection{Using pseudo-random number generators inside a random algorithm}
Randomized algorithms are algorithms that are allowed to flip
coins. Based on the assumption that the coins are IID RVs we can
calculate the expected running time of the algorithm, or the
probability that the algorithm generates a correct output.

But when we actually use the algorithm we use a pseudo-random number
generator, which is not the same as a real random sequence. So how do
we know that the analysis still works?

Well, suppose for example that the expected running time of the
randomized algorithm is much larger when it uses a pseudo-random
number generator then when it uses a true random generator. In that
case we have found a computationally efficient test for whether or not
the sequence is truly random or not - If the running time of the
algorithm is much larger than the expected running time (and assume
that the variance if the running time is small), then our test would
reject the hypothesis that the sequence is truly random.

We assume that our pseudo random number generator is such that no
efficiently computable test that can distinguish between the
pseudo-random output and a truly random sequence. As a result the
running time of our algorithm must be indistinguishable from the
running time when using a truly random sequence.


\subsection{Pseudo-Randomness and hash functions}
One of the most useful constructs in algorithm is the {\em hash
  function}. A hash function has two inputs: an {\em index} and a {\em
  key} and one output: the hash value. We denote the index by $i$, the
key by $k$ and the value of the hash function by $h_i(k)$.  Lets
assume that all three of these are binary vectors of length $n$,
i.e. elements of $\{0,1\}^n$

In order to use a hash function we randomly (or pseudo-randomly)
choose a value for the index. For this fixed index, we get a function
that maps keys to hash values. Obviously, if 

The main property that good hash functions need to have is that for
any fixed set of keys, if we choose the index at random, the values of
the hash function is random. Stated more precisely, the desired property is
the following:
\begin{itemize}
\item {\bf Singletons:} Considering any single key $k$ and any value
  $v$, the probability that $h_i(k)=v$, when $i$ is chosen uniformly
  at random, should be $2^{-n}$.
\item {\bf Pairs:} Considering any two different keys $k_1 \neq k_2$
  and any two values $v_1,v_2$. The probability that $h_i(k_1)=v_1$
  and $h_i(k_2)=v_2$ is $2^{-2n}$
\item {\bf General:} Considering any $l$ keys $k_1,\ldots,k_l$, all
  different, and $l$ values, the probability that $h_i(k_j)=v_j$ for
  all $1\leq j \leq l$ is $2^{-ln}$.
\end{itemize}

Hash functions are intimately connects to pseudo-random
generators. If we have a good pseudo-random generator we can use it to
construct a good hash function as follows.
\begin{enumerate}
\item Conatanate the index $i$ and the key $k$ to create a $2n$ bit
  number. Using this number as the seed of the generator.
\item Run the generator $n$ times to generate an $n$ bit
  number. Output this number as the value of the hash function $h_i(k)$.
\end{enumerate}

It is easy to verify that if the outputs of the hash function deviates from the
uniform distributions listed above then we have a way to detect that the
pseudo-random generator generates bits that are not distributed IID (1/2,1/2).


