\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Motivation}{7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}AB tests of online ads}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Probability Theory}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Low probability vs. certainty}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Average and Mean}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Monte Carlo Simulations}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces  Trajectories of running averages of random sequences generated by a biased coin with sides that say ``0'' and ``1''. There are two sets of 10 trajectories. The red trajectories correspond to sequences of random coin flips where the probability of 1 is $0.02$. The blue trajectories correspond to sequences of random coin flips where the probability of a 1 is $0.0175$}}{11}}
\newlabel{fig:Averages}{{1.1}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Summary}{12}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}A gentle introduction}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Probability and fair bets}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The event tree describing a single flip of a fair coin}}{14}}
\newlabel{fig:1Flip}{{2.1}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Event Trees}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Tossing a coin three times}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Specific outcome}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Event trees describing thre flips of a fair coin. {\bf  (a) Specified outcome:} events of interests are HHH and HHT. {\bf  (b) Number of tails:} events of interest are ``no tails'' (blue) and ``one tail'' (red). }}{15}}
\newlabel{fig:3Flips}{{2.2}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Specified number of tails}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}The three cards problem}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces {\bf  event tree describing the three cards problem.} The six nodes at the bottom level represent the six elements of the outcome space. The arrow up symbol $\wedge $ points to the side of the card that is facing up.}}{16}}
\newlabel{fig:3cards}{{2.3}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Chapter Summary}{17}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Combinatorics}{19}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Sets}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Spaces and complements}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Tuples, and products of sets}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}The size of a set}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Permutations and combinations}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Sampling with and without replacement when the order matters}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}When the order doesn't matter}{21}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Finite Uniform Probability spaces}{23}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Definition}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}A first set of canonical examples}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Tossing a fair coin}{25}}
\newlabel{sec:FairCoin}{{4.3}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Toss a fair coin 10 times}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Toss a fair coin $n$ times}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}A second set of canonical examples}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Poker}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Rules of Texas Hold'em}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Poker Hands}{30}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}General Probability Spaces}{31}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Non-Uniform Distributions over Finite Sample Spaces}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Wheels of chance: the wheel is given a push and spins until is comes to rest. The outcome is the label of the section of the wheel pointed to by the red arrows. {\bf  (a):} A wheel with four sections of different sizes. {\bf  (b):} A wheel with an infinite number of sections, labeled with the natural numbers $1,2,3,ldots$. }}{32}}
\newlabel{fig:Wheel-of-chance}{{5.1}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Distributions over the natural numbers}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Tossing a biased coin}{33}}
\newlabel{sec:BaisedCoin}{{5.3}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Density distributions}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces {\bf  Todo:} write a caption for the figure. }}{34}}
\newlabel{fig:Wheel-of-chance-uncountable}{{5.2}{34}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Mixture distributions}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}The limitations of histograms}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Cumulative Distribution Functions}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Examples of distributions on the real line}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces This figure depicts an example of mapping a random variable $X$ whose distribution is defined by a CDF (thick black curve) to another random variable $Y$, whose distribution is uniform in the interval $[0,1]$. The mapping uses the CDF as a function so that $Y=\unhbox \voidb@x \hbox {CDF}(X)$. }}{36}}
\newlabel{fig:CDFmap}{{5.3}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {5.9}Throwing a dart at a dartboard}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The CDFs for two distributions: (1) A point-mass distribution is a distribution that assignes non zero probabilities to two real values (marked by red circles) all sets that do not include at least one of these points have probability zero. The CDF of a point mass distribution is constant every where but at the point masses where it is discontinuous, the size of the discontinuity is the probability of the corresponding point. (2) A density distribution assigns probability zero to any single point. The CDF for such a distribution is a continuous increasing function that has a derivative. This derivative, $p(a)$ is called the {\em  density function} of the distribution. For density distributions the CDF and the density function contain the same information.}}{37}}
\newlabel{fig:CDF}{{5.4}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces {\bf  Left:} A discrete distribution concentrated on a single point $P(X=2)=1$. We denote this distribution by $PM(2)$. {\bf  Right:} A discrete distribution distributed evenly over the four points $-1,0,1,2$. This distribution can be expressed as $(PM(-1)+PM(0)+PM(1)+PM(2))/4$.}}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces {\bf  Left:} A non-uniform discrete distribution. This distribution can be expressed as $(1/4)PM(-1)+(1/8)PM(0)+(5/8)PM(1)+(1/4)PM(2)$. {\bf  Right:} The normal distribution with mean $0$ and varriance 1, denoted ${\cal  N}(0,1)$. This is a density distribution and it's density function is $f(x) = \frac  {1}{\sqrt  {2\pi }} \qopname  \relax o{exp}(-x^2/2)$.}}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces {\bf  Left:} A uniform distribution between $-3$ and $1$. We denote this distribution by $U(-3,1)$. {\bf  Right:} A mixture of two uniform distributions: $(U(-3,1)+U(2,3))/2$.}}{38}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Multiple events, conditioning, and independence}{39}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Two or more events on the same sample space}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Human intuition about combined event probabilities is fallible}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Joint probability tables}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}The union bound}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Balls in bins, or urn problems}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Conditional probability}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}A few examples}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}The summation rule}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}The Monty Hall problem}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.4}Another summation rule}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.5}Sex bias in graduate admissions}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Bayes' rule}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Independence}{46}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Random variables, expectation, and variance}{49}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Random variables}{49}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Finite, Infinite, and Undefined Expectations}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Finite Expectation}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Infinite Expectation}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Undefined Expectation}{50}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Arithmetic and Geometric Series}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}The mean, or expected value}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.1}An application to sampling}{54}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Linearity of expectation}{54}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Fixed points of a permutation}{55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Coupon collector, again}{55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.3}Balls in bins, again}{55}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Independent random variables}{56}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Variance}{56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.1}Properties of the variance}{57}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.2}Examples}{57}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.3}Another property of the variance}{59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.4}Linearity of variance}{59}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Covariance, correlation and dependence}{61}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{eqn:independentRVs}{{8.1}{61}}
\newlabel{EXY:1}{{8.2}{61}}
\newlabel{EXY:2}{{8.3}{61}}
\newlabel{EXY:3}{{8.4}{61}}
\newlabel{EXY:4}{{8.5}{61}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces  Examples of joint distributions and their corresponding correlation coefficients. The distributions are represented by a sample of points drawn IID from the underlying distribution. The number next to each cloud of points is the value of the correlation coefficients corresponding to the distribution. {\bf  The top row} shows ellipsoidal distributions with varying levels of correlation. {\bf  The middle row} shows distributions that are concentrated on a line of the form $X=aY$, the correlation for these distributions is one of $-1,0,+1$ depending on the value of $a$. {\bf  The bottom row} shows a variety of distributions where $X$ and $Y$ are uncorrelated. These distributions demonstrate some of the many ways in which uncorrelated random variables can be dependent.(Image taken from WikiPedia:Correlation Coefficients)}}{62}}
\newlabel{fig:corrCoeff}{{8.1}{62}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Sampling, hypothesis testing, and the central limit theorem}{65}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}The binomial distribution}{65}}
\newlabel{eq:bin-2sigma}{{9.1}{66}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Hypothesis testing}{66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Testing a vaccine}{66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}A blood pressure drug}{66}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Sampling}{67}}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}The normal distribution}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.1}Sums of independent random variables}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.2}Tails of the normal}{69}}
\newlabel{fact:normal-tails}{{6}{69}}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Sampling revisited}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.1}Polls with yes/no answers}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.2}Polls with numeric answers}{70}}
\@writefile{toc}{\contentsline {section}{\numberline {9.6}The Bonferroni correction}{71}}
\@writefile{toc}{\contentsline {section}{\numberline {9.7}Estimating the size of large populations}{71}}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Randomized algorithms}{73}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}Finding percentiles}{73}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.1}The mean as a summary statistic}{73}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.2}Selection}{74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.3}A randomized algorithm}{74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.4}Running time analysis}{75}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.5}A randomized sorting algorithm}{75}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.6}Two types of randomized algorithms}{76}}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}Sorting in expected linear time}{76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.1}Worst-case lower bound on sorting}{76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.2}Sorting in expected linear time}{77}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.3}Sorting in linear time when the distribution is known}{78}}
\newlabel{eqn:CDF}{{10.1}{78}}
\@writefile{toc}{\contentsline {section}{\numberline {10.3}Karger's minimum cut algorithm}{78}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.1}Clustering via graph cuts}{78}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.2}Karger's algorithm}{79}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.3}Analysis}{79}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces Karger's algorithm at work.}}{80}}
\newlabel{fig:karger}{{10.1}{80}}
\@writefile{toc}{\contentsline {section}{\numberline {10.4}Hashing}{81}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.1}The hashing framework}{81}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.2}A simple solution using randomization}{82}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.3}Average query time}{82}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.4}Worst case query time, and a balls-in-bins problem}{82}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.5}The power of two choices}{83}}
\@writefile{toc}{\contentsline {section}{\numberline {10.5}Information retrieval}{83}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.1}Preprocessing}{84}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.2}Answering a query}{84}}
\@writefile{toc}{\contentsline {section}{\numberline {10.6}Detecting near-duplicates}{85}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6.1}The similarity between two documents}{85}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6.2}An algorithm based on random permutations}{86}}
\@writefile{toc}{\contentsline {section}{\numberline {10.7}Bloom Filters}{87}}
\newlabel{eqn:optimal-k}{{10.2}{88}}
\newlabel{eqn:optimal-p}{{10.3}{88}}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Pseudo-randomness}{91}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}Randomness and predictability}{91}}
\@writefile{toc}{\contentsline {section}{\numberline {11.2}Pseudo-Randomness}{91}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.1}Predicting a pseudo-random generator}{92}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.2}Computationally bounded randomness}{92}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.3}Using pseudo-random number generators inside a random algorithm}{93}}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Random generation}{95}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}Simulating simple discrete distributions with a fair coin}{95}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.1}Uniform distribution over $b$-bit integers}{95}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.2}Uniform distribution over $\{1,2,\ldots  , n\}$}{95}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.3}Uniform distribution over $[0,1]$}{96}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.4}Generating flips of a biased coin}{96}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.5}Arbitrary discrete distribution with finite sample space}{97}}
\@writefile{toc}{\contentsline {section}{\numberline {12.2}From biased coin to fair coin}{97}}
\@writefile{toc}{\contentsline {section}{\numberline {12.3}Random permutations}{98}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.1}Implications}{99}}
\@writefile{toc}{\contentsline {chapter}{\numberline {13}Machine learning}{101}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {13.1}Nearest neighbor classification}{101}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.1}Digit recognition}{101}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.2}The input space and the label space}{101}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.3}A nearest neighbor classifier}{102}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.4}Two improvements}{102}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.5}The computational complexity of finding the nearest neighbor}{102}}
\@writefile{toc}{\contentsline {section}{\numberline {13.2}Decision trees}{103}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.1}Credit card fraud detection}{103}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.2}The input space and the label space}{103}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.3}Classification by decision tree}{104}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.4}Learning a decision tree}{104}}
\@writefile{toc}{\contentsline {section}{\numberline {13.3}Linear classifiers}{105}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.1}Document classification}{105}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.2}The input space ${\cal  X}$ and label space ${\cal  Y}$}{106}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.3}Linear classifiers}{106}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.4}Learning a linear classifier}{107}}
