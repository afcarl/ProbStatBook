\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Statistics}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Probability Theory}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Low probability vs. certainty}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Average and Mean}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Monte-carlo simulations}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces  Trajectories of running averages of random sequences generated by a biased coin with sides that say ``0'' and ``1''. There are two sets of 10 trajectories. The red trajectories correspond to sequences of random coin flips where the probability of 1 is $0.02$. The blue trajectories correspond to sequences of random coin flips where the probability of a 1 is $0.0175$}}{10}}
\newlabel{fig:Averages}{{1.1}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Summary}{11}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Mathematical preliminaries}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Sets}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Spaces and complements}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Tuples, and products of sets}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}The size of a set}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Permutations and combinations}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Sampling with and without replacement when the order matters}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}When the order doesn't matter}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Arithmetic and geometric series}{16}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Probability spaces}{17}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Definition}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}A first set of canonical examples}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Tossing a fair coin}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Toss a fair coin 10 times}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Toss a fair coin $n$ times}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}A second set of canonical examples}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Tossing a biased coin}{23}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Multiple events, conditioning, and independence}{25}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Two or more events on the same sample space}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Human intuition about combined event probabilities is fallible}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Joint probability tables}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}The union bound}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Conditional probability}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}A few examples}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}The summation rule}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Tossing a biased coin}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Conditional probability, continued}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Another summation rule}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Sex bias in graduate admissions}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Bayes' rule}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Independence}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}The Monty Hall problem}{31}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Random variables, expectation, and variance, I}{33}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Random variables}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}The mean, or expected value}{34}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Tossing a biased coin}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}An application to sampling}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Linearity of expectation}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Fixed points of a permutation}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Coupon collector, again}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Balls in bins, again}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Independent random variables}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Variance}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}Properties of the variance}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Examples}{40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.3}Another property of the variance}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.4}Linearity of variance}{42}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Sampling, hypothesis testing, and the central limit theorem}{43}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}The binomial distribution}{43}}
\newlabel{eq:bin-2sigma}{{6.1}{44}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Hypothesis testing}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Testing a vaccine}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}A blood pressure drug}{44}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Sampling}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}The normal distribution}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Sums of independent random variables}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Tails of the normal}{47}}
\newlabel{fact:normal-tails}{{6}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Sampling revisited}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Polls with yes/no answers}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Polls with numeric answers}{48}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Multiple Hypothesis testing, Covariance, Correlation}{51}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}The Bonferroni correction}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Catch, mark and release}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Covariance}{52}}
\newlabel{eqn:independentRVs}{{7.1}{52}}
\newlabel{EXY:1}{{7.2}{53}}
\newlabel{EXY:2}{{7.3}{53}}
\newlabel{EXY:3}{{7.4}{53}}
\newlabel{EXY:4}{{7.5}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces  Examples of joint distributions and their corresponding correlation coefficients. The distributions are represented by a sample of points drawn IID from the underlying distribution. The number next to each cloud of points is the value of the correlation coefficients corresponding to the distribution. {\bf  The top row} shows ellipsoidal distributions with varying levels of correlation. {\bf  The middle row} shows distributions that are concentrated on a line of the form $X=aY$, the correlation for these distributions is one of $-1,0,+1$ depending on the value of $a$. {\bf  The bottom row} shows a variety of distributions where $X$ and $Y$ are uncorrelated. These distributions demonstrate some of the many ways in which uncorrelated random variables can be dependent.(Image taken from WikiPedia:Correlation Coefficients)}}{54}}
\newlabel{fig:corrCoeff}{{7.1}{54}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Pseudo-randomness}{55}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Randomness and predictability}{55}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Pseudo-Randomness}{55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Predicting a pseudo-random generator}{56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Computationally bounded randomness}{56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}Using pseudo-random number generators inside a random algorithm}{57}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Randomized algorithms}{59}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Finding percentiles}{59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}The mean as a summary statistic}{59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.2}Selection}{60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.3}A randomized algorithm}{60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.4}Running time analysis}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.5}A randomized sorting algorithm}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.6}Two types of randomized algorithms}{62}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Cumulative distributions and sorting in expected linear time}{62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Cumulative Distribution Functions}{62}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces This figure depicts an example of mapping a random variable $X$ whose distribution is defined by a CDF (thick black curve) to another random variable $Y$, whose distribution is uniform in the interval $[0,1]$. The mapping uses the CDF as a function so that $Y=\unhbox \voidb@x \hbox {CDF}(X)$. }}{63}}
\newlabel{fig:CDFmap}{{9.1}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}Examples of distributions on the real line}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.3}Worst-case lower bound on sorting}{63}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces The CDFs for two distributions: (1) A point-mass distribution is a distribution that assignes non zero probabilities to two real values (marked by red circles) all sets that do not include at least one of these points have probability zero. The CDF of a point mass distribution is constant every where but at the point masses where it is discontinuous, the size of the discontinuity is the probability of the corresponding point. (2) A density distribution assigns probability zero to any single point. The CDF for such a distribution is a continuous increasing function that has a derivative. This derivative, $p(a)$ is called the {\em  density function} of the distribution. For density distributions the CDF and the density function contain the same information.}}{64}}
\newlabel{fig:CDF}{{9.2}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces {\bf  Left:} A discrete distribution concentrated on a single point $P(X=2)=1$. We denote this distribution by $PM(2)$. {\bf  Right:} A discrete distribution distributed evenly over the four points $-1,0,1,2$. This distribution can be expressed as $(PM(-1)+PM(0)+PM(1)+PM(2))/4$.}}{65}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.4}Sorting in expected linear time}{65}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.5}Sorting in linear time when the distribution is known}{66}}
\newlabel{eqn:CDF}{{9.1}{66}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Karger's minimum cut algorithm}{67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}Clustering via graph cuts}{67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}Karger's algorithm}{67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.3}Analysis}{67}}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Hashing}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.1}The hashing framework}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.2}A simple solution using randomization}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.3}Average query time}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.4}Worst case query time, and a balls-in-bins problem}{70}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.5}The power of two choices}{70}}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Information retrieval}{71}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.1}Preprocessing}{71}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.2}Answering a query}{72}}
\@writefile{toc}{\contentsline {section}{\numberline {9.6}Detecting near-duplicates}{73}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6.1}The similarity between two documents}{73}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6.2}An algorithm based on random permutations}{74}}
\@writefile{toc}{\contentsline {section}{\numberline {9.7}Bloom Filters}{75}}
\newlabel{eqn:optimal-k}{{9.2}{76}}
\newlabel{eqn:optimal-p}{{9.3}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces {\bf  Left:} A non-uniform discrete distribution. This distribution can be expressed as $(1/4)PM(-1)+(1/8)PM(0)+(5/8)PM(1)+(1/4)PM(2)$. {\bf  Right:} The normal distribution with mean $0$ and varriance 1, denoted ${\cal  N}(0,1)$. This is a density distribution and it's density function is $f(x) = \frac  {1}{\sqrt  {2\pi }} \qopname  \relax o{exp}(-x^2/2)$.}}{78}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces {\bf  Left:} A uniform distribution between $-3$ and $1$. We denote this distribution by $U(-3,1)$. {\bf  Right:} A mixture of two uniform distributions: $(U(-3,1)+U(2,3))/2$.}}{78}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.6}{\ignorespaces Karger's algorithm at work.}}{79}}
\newlabel{fig:karger}{{9.6}{79}}
